*** SLURM BATCH JOB 'transformers' STARTING ***
*** Activating environment 236004_transformers_project ***
CUDA flag: True
Device: cuda
All logs will be located in: logs
This experiment's logs will be located in: logs/logs/-date=2025_06_13_21:03:25
wandb: Currently logged in as: amitvais777 (amitvais777-technion-israel-institute-of-technology) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: WARNING Using a boolean value for 'reinit' is deprecated. Use 'return_previous' or 'finish_previous' instead.
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /home/amit.vaisman/236004_transformers_project/wandb/run-20250613_210326-cmfu5nq4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run -date=2025_06_13_21:03:25
wandb: ‚≠êÔ∏è View project at https://wandb.ai/amitvais777-technion-israel-institute-of-technology/uncategorized
wandb: üöÄ View run at https://wandb.ai/amitvais777-technion-israel-institute-of-technology/uncategorized/runs/cmfu5nq4
Loading data...
--------------------------------------------------
Loading WordNet TargetKG...
Train TargetKG len:  16
/home/amit.vaisman/miniconda3/envs/236004_transformers_project/lib/python3.10/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Loading WordNet TargetKG done.
--------------------------------------------------
Loading WordNet ControlKG...
Train ControlKG len:  9707
Val ControlKG len:  50
Loading WordNet ControlKG done.
--------------------------------------------------
Loading wikitext2 as ControlLM...
Running tokenizer on ControlLM dataset:   0%|          | 0/4358 [00:00<?, ? examples/s]Running tokenizer on ControlLM dataset:  23%|‚ñà‚ñà‚ñé       | 1000/4358 [00:00<00:00, 4031.33 examples/s]Running tokenizer on ControlLM dataset:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 2000/4358 [00:00<00:00, 5456.29 examples/s]Running tokenizer on ControlLM dataset:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 3000/4358 [00:00<00:00, 6145.34 examples/s]Running tokenizer on ControlLM dataset:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 4000/4358 [00:00<00:00, 6977.26 examples/s]Running tokenizer on ControlLM dataset: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4358/4358 [00:00<00:00, 6406.43 examples/s]
Running tokenizer on ControlLM dataset:   0%|          | 0/36718 [00:00<?, ? examples/s]Running tokenizer on ControlLM dataset:   3%|‚ñé         | 1000/36718 [00:00<00:04, 8238.68 examples/s]Running tokenizer on ControlLM dataset:   5%|‚ñå         | 2000/36718 [00:00<00:04, 7452.44 examples/s]Running tokenizer on ControlLM dataset:   8%|‚ñä         | 3000/36718 [00:00<00:04, 7844.40 examples/s]Running tokenizer on ControlLM dataset:  11%|‚ñà         | 4000/36718 [00:00<00:04, 7861.97 examples/s]Running tokenizer on ControlLM dataset:  14%|‚ñà‚ñé        | 5000/36718 [00:00<00:03, 8318.91 examples/s]Running tokenizer on ControlLM dataset:  16%|‚ñà‚ñã        | 6000/36718 [00:00<00:03, 8541.88 examples/s]Running tokenizer on ControlLM dataset:  19%|‚ñà‚ñâ        | 7000/36718 [00:00<00:03, 7882.75 examples/s]Running tokenizer on ControlLM dataset:  22%|‚ñà‚ñà‚ñè       | 8000/36718 [00:01<00:03, 7752.22 examples/s]Running tokenizer on ControlLM dataset:  25%|‚ñà‚ñà‚ñç       | 9000/36718 [00:01<00:03, 7830.51 examples/s]Running tokenizer on ControlLM dataset:  27%|‚ñà‚ñà‚ñã       | 10000/36718 [00:01<00:03, 7923.34 examples/s]Running tokenizer on ControlLM dataset:  30%|‚ñà‚ñà‚ñâ       | 11000/36718 [00:01<00:03, 7843.12 examples/s]Running tokenizer on ControlLM dataset:  33%|‚ñà‚ñà‚ñà‚ñé      | 12000/36718 [00:01<00:03, 7932.00 examples/s]Running tokenizer on ControlLM dataset:  35%|‚ñà‚ñà‚ñà‚ñå      | 13000/36718 [00:01<00:03, 6042.15 examples/s]Running tokenizer on ControlLM dataset:  38%|‚ñà‚ñà‚ñà‚ñä      | 14000/36718 [00:01<00:03, 6555.49 examples/s]Running tokenizer on ControlLM dataset:  41%|‚ñà‚ñà‚ñà‚ñà      | 15000/36718 [00:02<00:03, 7018.56 examples/s]Running tokenizer on ControlLM dataset:  44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 16000/36718 [00:02<00:02, 7025.64 examples/s]Running tokenizer on ControlLM dataset:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 17000/36718 [00:02<00:02, 7211.71 examples/s]Running tokenizer on ControlLM dataset:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 18000/36718 [00:02<00:02, 7457.97 examples/s]Running tokenizer on ControlLM dataset:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 19000/36718 [00:02<00:02, 7810.89 examples/s]Running tokenizer on ControlLM dataset:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 20000/36718 [00:02<00:02, 7909.24 examples/s]Running tokenizer on ControlLM dataset:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 21000/36718 [00:02<00:02, 7806.09 examples/s]Running tokenizer on ControlLM dataset:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 22000/36718 [00:02<00:01, 7761.58 examples/s]Running tokenizer on ControlLM dataset:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 23000/36718 [00:03<00:01, 7921.37 examples/s]Running tokenizer on ControlLM dataset:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 24000/36718 [00:03<00:01, 8258.31 examples/s]Running tokenizer on ControlLM dataset:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 25000/36718 [00:03<00:01, 8276.99 examples/s]Running tokenizer on ControlLM dataset:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 26000/36718 [00:03<00:01, 8164.12 examples/s]Running tokenizer on ControlLM dataset:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 27000/36718 [00:03<00:01, 7862.33 examples/s]Running tokenizer on ControlLM dataset:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 28000/36718 [00:03<00:01, 7688.67 examples/s]Running tokenizer on ControlLM dataset:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 29000/36718 [00:03<00:00, 7977.61 examples/s]Running tokenizer on ControlLM dataset:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 30000/36718 [00:03<00:00, 7953.07 examples/s]Running tokenizer on ControlLM dataset:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 31000/36718 [00:04<00:00, 7495.43 examples/s]Running tokenizer on ControlLM dataset:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 32000/36718 [00:04<00:00, 7434.94 examples/s]Running tokenizer on ControlLM dataset:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 33000/36718 [00:04<00:00, 7410.31 examples/s]Running tokenizer on ControlLM dataset:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 34000/36718 [00:04<00:00, 7441.96 examples/s]Running tokenizer on ControlLM dataset:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 35000/36718 [00:04<00:00, 7643.35 examples/s]Running tokenizer on ControlLM dataset:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 36000/36718 [00:04<00:00, 7513.73 examples/s]Running tokenizer on ControlLM dataset: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 36718/36718 [00:04<00:00, 7429.44 examples/s]
Running tokenizer on ControlLM dataset:   0%|          | 0/3760 [00:00<?, ? examples/s]Running tokenizer on ControlLM dataset:  27%|‚ñà‚ñà‚ñã       | 1000/3760 [00:00<00:00, 9043.60 examples/s]Running tokenizer on ControlLM dataset:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 2000/3760 [00:00<00:00, 7812.81 examples/s]Running tokenizer on ControlLM dataset:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 3000/3760 [00:00<00:00, 7526.90 examples/s]Running tokenizer on ControlLM dataset: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3760/3760 [00:00<00:00, 7676.53 examples/s]
Grouping texts in chunks of 512:   0%|          | 0/4358 [00:00<?, ? examples/s]Grouping texts in chunks of 512:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 2000/4358 [00:00<00:00, 13026.76 examples/s]Grouping texts in chunks of 512:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 4000/4358 [00:00<00:00, 13625.18 examples/s]Grouping texts in chunks of 512: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4358/4358 [00:00<00:00, 13560.46 examples/s]
Grouping texts in chunks of 512:   0%|          | 0/36718 [00:00<?, ? examples/s]Grouping texts in chunks of 512:   5%|‚ñå         | 2000/36718 [00:00<00:02, 13128.47 examples/s]Grouping texts in chunks of 512:  11%|‚ñà         | 4000/36718 [00:00<00:02, 13658.10 examples/s]Grouping texts in chunks of 512:  16%|‚ñà‚ñã        | 6000/36718 [00:00<00:02, 14606.10 examples/s]Grouping texts in chunks of 512:  22%|‚ñà‚ñà‚ñè       | 8000/36718 [00:00<00:02, 13746.48 examples/s]Grouping texts in chunks of 512:  27%|‚ñà‚ñà‚ñã       | 10000/36718 [00:00<00:01, 13826.26 examples/s]Grouping texts in chunks of 512:  33%|‚ñà‚ñà‚ñà‚ñé      | 12000/36718 [00:00<00:01, 13788.88 examples/s]Grouping texts in chunks of 512:  38%|‚ñà‚ñà‚ñà‚ñä      | 14000/36718 [00:01<00:01, 13803.25 examples/s]Grouping texts in chunks of 512:  44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 16000/36718 [00:01<00:01, 13672.07 examples/s]Grouping texts in chunks of 512:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 18000/36718 [00:01<00:01, 13673.63 examples/s]Grouping texts in chunks of 512:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 20000/36718 [00:01<00:01, 13943.51 examples/s]Grouping texts in chunks of 512:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 22000/36718 [00:01<00:01, 13674.55 examples/s]Grouping texts in chunks of 512:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 24000/36718 [00:01<00:00, 14161.70 examples/s]Grouping texts in chunks of 512:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 26000/36718 [00:01<00:00, 14292.27 examples/s]Grouping texts in chunks of 512:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 28000/36718 [00:02<00:00, 13736.06 examples/s]Grouping texts in chunks of 512:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 30000/36718 [00:02<00:00, 13997.31 examples/s]Grouping texts in chunks of 512:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 32000/36718 [00:02<00:00, 13494.22 examples/s]Grouping texts in chunks of 512:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 34000/36718 [00:02<00:00, 13293.14 examples/s]Grouping texts in chunks of 512:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 36000/36718 [00:02<00:00, 13329.80 examples/s]Grouping texts in chunks of 512: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 36718/36718 [00:02<00:00, 13404.21 examples/s]
Grouping texts in chunks of 512:   0%|          | 0/3760 [00:00<?, ? examples/s]Grouping texts in chunks of 512:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 2000/3760 [00:00<00:00, 13481.54 examples/s]Grouping texts in chunks of 512: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3760/3760 [00:00<00:00, 13319.81 examples/s]Grouping texts in chunks of 512: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3760/3760 [00:00<00:00, 13242.69 examples/s]
Train ControlLM len: 4656
Val ControlLM len: 250
Loading ControlLM done.
--------------------------------------------------
Logging dataset for quality check...
wandb: WARNING A graphql request initiated by the public wandb API timed out (timeout=19 sec). Create a new API with an integer timeout larger than 19, e.g., `api = wandb.Api(timeout=29)` to increase the graphql timeout.
Dataset loading and saving done.
Prune - (out,in)_w_per_mask: (1, 1)
Replaced c_attn in h.0.attn
Replaced c_proj in h.0.attn
Replaced c_proj in h.0.mlp
Replaced c_fc in h.0.mlp
Replaced c_attn in h.1.attn
Replaced c_proj in h.1.attn
Replaced c_proj in h.1.mlp
Replaced c_fc in h.1.mlp
Replaced c_attn in h.2.attn
Replaced c_proj in h.2.attn
Replaced c_proj in h.2.mlp
Replaced c_fc in h.2.mlp
Replaced c_attn in h.3.attn
Replaced c_proj in h.3.attn
Replaced c_proj in h.3.mlp
Replaced c_fc in h.3.mlp
Replaced c_attn in h.4.attn
Replaced c_proj in h.4.attn
Replaced c_proj in h.4.mlp
Replaced c_fc in h.4.mlp
Replaced c_attn in h.5.attn
Replaced c_proj in h.5.attn
Replaced c_proj in h.5.mlp
Replaced c_fc in h.5.mlp
Replaced c_attn in h.6.attn
Replaced c_proj in h.6.attn
Replaced c_proj in h.6.mlp
Replaced c_fc in h.6.mlp
Replaced c_attn in h.7.attn
Replaced c_proj in h.7.attn
Replaced c_proj in h.7.mlp
Replaced c_fc in h.7.mlp
Replaced c_attn in h.8.attn
Replaced c_proj in h.8.attn
Replaced c_proj in h.8.mlp
Replaced c_fc in h.8.mlp
Replaced c_attn in h.9.attn
Replaced c_proj in h.9.attn
Replaced c_proj in h.9.mlp
Replaced c_fc in h.9.mlp
Replaced c_attn in h.10.attn
Replaced c_proj in h.10.attn
Replaced c_proj in h.10.mlp
Replaced c_fc in h.10.mlp
Replaced c_attn in h.11.attn
Replaced c_proj in h.11.attn
Replaced c_proj in h.11.mlp
Replaced c_fc in h.11.mlp
lambda_reg_init: 2.0, lambda_reg_final: 3.0
lr_base: 5e-05, mask_lr_base: 0.2, lr_warmup_frac: 0.1, epochs: 40000, batch_size: 250
Mask params are trainable:  True
Model params are frozen:  True


eval: : 0it [00:00, ?it/s][A[A

eval: : 1it [00:00,  1.46it/s][A[A

                              [A[A

eval: : 0it [00:00, ?it/s][A[A

                          [A[A

eval: : 0it [00:00, ?it/s][A[A

                          [A[ATraceback (most recent call last):
  File "/home/amit.vaisman/236004_transformers_project/subnet_train.py", line 242, in <module>
    main()
  File "/home/amit.vaisman/236004_transformers_project/subnet_train.py", line 214, in main
    last_log_dict, model = train_mask(
  File "/home/amit.vaisman/236004_transformers_project/know_subnet/subnet_train_utils.py", line 761, in train_mask
    log_dict = zeroshot_log_loop(
  File "/home/amit.vaisman/miniconda3/envs/236004_transformers_project/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/amit.vaisman/236004_transformers_project/know_subnet/subnet_train_utils.py", line 167, in zeroshot_log_loop
    metric_dict = test_mask(
  File "/home/amit.vaisman/miniconda3/envs/236004_transformers_project/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/amit.vaisman/236004_transformers_project/know_subnet/subnet_train_utils.py", line 89, in test_mask
    top1_acc, top5_acc, top10_acc, num_masked_tokens = acc_func(logits, labels, mask)
  File "/home/amit.vaisman/236004_transformers_project/know_subnet/metrics.py", line 72, in acc_func
    masked_prbs = prbs[mask,:]
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 11.96 GiB (GPU 0; 47.44 GiB total capacity; 43.34 GiB already allocated; 1.02 GiB free; 46.10 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "/home/amit.vaisman/236004_transformers_project/subnet_train.py", line 242, in <module>
    main()
  File "/home/amit.vaisman/236004_transformers_project/subnet_train.py", line 214, in main
    last_log_dict, model = train_mask(
  File "/home/amit.vaisman/236004_transformers_project/know_subnet/subnet_train_utils.py", line 761, in train_mask
    log_dict = zeroshot_log_loop(
  File "/home/amit.vaisman/miniconda3/envs/236004_transformers_project/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/amit.vaisman/236004_transformers_project/know_subnet/subnet_train_utils.py", line 167, in zeroshot_log_loop
    metric_dict = test_mask(
  File "/home/amit.vaisman/miniconda3/envs/236004_transformers_project/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/amit.vaisman/236004_transformers_project/know_subnet/subnet_train_utils.py", line 89, in test_mask
    top1_acc, top5_acc, top10_acc, num_masked_tokens = acc_func(logits, labels, mask)
  File "/home/amit.vaisman/236004_transformers_project/know_subnet/metrics.py", line 72, in acc_func
    masked_prbs = prbs[mask,:]
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 11.96 GiB (GPU 0; 47.44 GiB total capacity; 43.34 GiB already allocated; 1.02 GiB free; 46.10 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[1;34mwandb[0m: 
[1;34mwandb[0m: üöÄ View run [33m-date=2025_06_13_21:03:25[0m at: [34mhttps://wandb.ai/amitvais777-technion-israel-institute-of-technology/uncategorized/runs/cmfu5nq4[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20250613_210326-cmfu5nq4/logs[0m
*** SLURM BATCH JOB 'transformers' DONE ***
