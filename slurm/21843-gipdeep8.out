*** SLURM BATCH JOB 'transformers' STARTING ***
*** Activating environment 236004_transformers_project ***
CUDA flag: True
Device: cuda
All logs will be located in: logs
This experiment's logs will be located in: logs/logs/-date=2025_06_14_13:13:29
wandb: Currently logged in as: amitvais777 (amitvais777-technion-israel-institute-of-technology) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: WARNING Using a boolean value for 'reinit' is deprecated. Use 'return_previous' or 'finish_previous' instead.
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /home/amit.vaisman/236004_transformers_project/wandb/run-20250614_131330-mfliuk5v
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run -date=2025_06_14_13:13:29
wandb: ‚≠êÔ∏è View project at https://wandb.ai/amitvais777-technion-israel-institute-of-technology/uncategorized
wandb: üöÄ View run at https://wandb.ai/amitvais777-technion-israel-institute-of-technology/uncategorized/runs/mfliuk5v
Loading data...
--------------------------------------------------
Loading WordNet TargetKG...
Train TargetKG len:  16
/home/amit.vaisman/miniconda3/envs/236004_transformers_project/lib/python3.10/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Loading WordNet TargetKG done.
--------------------------------------------------
Loading WordNet ControlKG...
Train ControlKG len:  9707
Val ControlKG len:  50
Loading WordNet ControlKG done.
--------------------------------------------------
Loading wikitext2 as ControlLM...
Running tokenizer on ControlLM dataset:   0%|          | 0/4358 [00:00<?, ? examples/s]Running tokenizer on ControlLM dataset:  23%|‚ñà‚ñà‚ñé       | 1000/4358 [00:00<00:00, 4026.17 examples/s]Running tokenizer on ControlLM dataset:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 2000/4358 [00:00<00:00, 4247.43 examples/s]Running tokenizer on ControlLM dataset:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 3000/4358 [00:00<00:00, 4810.83 examples/s]Running tokenizer on ControlLM dataset:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 4000/4358 [00:00<00:00, 5873.38 examples/s]Running tokenizer on ControlLM dataset: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4358/4358 [00:00<00:00, 5411.71 examples/s]
Running tokenizer on ControlLM dataset:   0%|          | 0/36718 [00:00<?, ? examples/s]Running tokenizer on ControlLM dataset:   3%|‚ñé         | 1000/36718 [00:00<00:04, 8155.44 examples/s]Running tokenizer on ControlLM dataset:   5%|‚ñå         | 2000/36718 [00:00<00:04, 7410.55 examples/s]Running tokenizer on ControlLM dataset:   8%|‚ñä         | 3000/36718 [00:00<00:04, 7826.69 examples/s]Running tokenizer on ControlLM dataset:  11%|‚ñà         | 4000/36718 [00:00<00:04, 7831.01 examples/s]Running tokenizer on ControlLM dataset:  14%|‚ñà‚ñé        | 5000/36718 [00:00<00:03, 8282.35 examples/s]Running tokenizer on ControlLM dataset:  16%|‚ñà‚ñã        | 6000/36718 [00:00<00:03, 8501.11 examples/s]Running tokenizer on ControlLM dataset:  19%|‚ñà‚ñâ        | 7000/36718 [00:00<00:03, 7847.10 examples/s]Running tokenizer on ControlLM dataset:  22%|‚ñà‚ñà‚ñè       | 8000/36718 [00:01<00:03, 7726.55 examples/s]Running tokenizer on ControlLM dataset:  25%|‚ñà‚ñà‚ñç       | 9000/36718 [00:01<00:03, 7784.81 examples/s]Running tokenizer on ControlLM dataset:  27%|‚ñà‚ñà‚ñã       | 10000/36718 [00:01<00:03, 7858.73 examples/s]Running tokenizer on ControlLM dataset:  30%|‚ñà‚ñà‚ñâ       | 11000/36718 [00:01<00:03, 7785.75 examples/s]Running tokenizer on ControlLM dataset:  33%|‚ñà‚ñà‚ñà‚ñé      | 12000/36718 [00:01<00:03, 7899.20 examples/s]Running tokenizer on ControlLM dataset:  35%|‚ñà‚ñà‚ñà‚ñå      | 13000/36718 [00:01<00:03, 6088.20 examples/s]Running tokenizer on ControlLM dataset:  38%|‚ñà‚ñà‚ñà‚ñä      | 14000/36718 [00:01<00:03, 6592.01 examples/s]Running tokenizer on ControlLM dataset:  41%|‚ñà‚ñà‚ñà‚ñà      | 15000/36718 [00:02<00:03, 7013.33 examples/s]Running tokenizer on ControlLM dataset:  44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 16000/36718 [00:02<00:02, 7021.18 examples/s]Running tokenizer on ControlLM dataset:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 17000/36718 [00:02<00:02, 7198.37 examples/s]Running tokenizer on ControlLM dataset:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 18000/36718 [00:02<00:02, 7438.66 examples/s]Running tokenizer on ControlLM dataset:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 19000/36718 [00:02<00:02, 7622.13 examples/s]Running tokenizer on ControlLM dataset:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 20000/36718 [00:02<00:02, 7759.38 examples/s]Running tokenizer on ControlLM dataset:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 21000/36718 [00:02<00:02, 7653.23 examples/s]Running tokenizer on ControlLM dataset:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 22000/36718 [00:02<00:01, 7618.27 examples/s]Running tokenizer on ControlLM dataset:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 23000/36718 [00:03<00:01, 7814.07 examples/s]Running tokenizer on ControlLM dataset:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 24000/36718 [00:03<00:01, 8161.63 examples/s]Running tokenizer on ControlLM dataset:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 25000/36718 [00:03<00:01, 8340.83 examples/s]Running tokenizer on ControlLM dataset:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 26000/36718 [00:03<00:01, 8187.12 examples/s]Running tokenizer on ControlLM dataset:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 27000/36718 [00:03<00:01, 7854.06 examples/s]Running tokenizer on ControlLM dataset:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 28000/36718 [00:03<00:01, 7638.93 examples/s]Running tokenizer on ControlLM dataset:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 29000/36718 [00:03<00:00, 7918.27 examples/s]Running tokenizer on ControlLM dataset:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 30000/36718 [00:03<00:00, 7898.00 examples/s]Running tokenizer on ControlLM dataset:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 31000/36718 [00:04<00:00, 7476.47 examples/s]Running tokenizer on ControlLM dataset:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 32000/36718 [00:04<00:00, 7454.96 examples/s]Running tokenizer on ControlLM dataset:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 33000/36718 [00:04<00:00, 7395.34 examples/s]Running tokenizer on ControlLM dataset:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 34000/36718 [00:04<00:00, 7411.22 examples/s]Running tokenizer on ControlLM dataset:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 35000/36718 [00:04<00:00, 7633.88 examples/s]Running tokenizer on ControlLM dataset:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 36000/36718 [00:04<00:00, 7495.12 examples/s]Running tokenizer on ControlLM dataset: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 36718/36718 [00:04<00:00, 7383.53 examples/s]
Running tokenizer on ControlLM dataset:   0%|          | 0/3760 [00:00<?, ? examples/s]Running tokenizer on ControlLM dataset:  27%|‚ñà‚ñà‚ñã       | 1000/3760 [00:00<00:00, 9096.38 examples/s]Running tokenizer on ControlLM dataset:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 2000/3760 [00:00<00:00, 7793.28 examples/s]Running tokenizer on ControlLM dataset:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 3000/3760 [00:00<00:00, 7518.90 examples/s]Running tokenizer on ControlLM dataset: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3760/3760 [00:00<00:00, 7668.61 examples/s]
Grouping texts in chunks of 512:   0%|          | 0/4358 [00:00<?, ? examples/s]Grouping texts in chunks of 512:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 2000/4358 [00:00<00:00, 13333.73 examples/s]Grouping texts in chunks of 512:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 4000/4358 [00:00<00:00, 13974.15 examples/s]Grouping texts in chunks of 512: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4358/4358 [00:00<00:00, 13895.15 examples/s]
Grouping texts in chunks of 512:   0%|          | 0/36718 [00:00<?, ? examples/s]Grouping texts in chunks of 512:   5%|‚ñå         | 2000/36718 [00:00<00:02, 13504.57 examples/s]Grouping texts in chunks of 512:  11%|‚ñà         | 4000/36718 [00:00<00:02, 13994.73 examples/s]Grouping texts in chunks of 512:  16%|‚ñà‚ñã        | 6000/36718 [00:00<00:02, 15020.26 examples/s]Grouping texts in chunks of 512:  22%|‚ñà‚ñà‚ñè       | 8000/36718 [00:00<00:02, 14131.22 examples/s]Grouping texts in chunks of 512:  27%|‚ñà‚ñà‚ñã       | 10000/36718 [00:00<00:01, 14200.14 examples/s]Grouping texts in chunks of 512:  33%|‚ñà‚ñà‚ñà‚ñé      | 12000/36718 [00:00<00:01, 14133.71 examples/s]Grouping texts in chunks of 512:  38%|‚ñà‚ñà‚ñà‚ñä      | 14000/36718 [00:00<00:01, 14138.49 examples/s]Grouping texts in chunks of 512:  44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 16000/36718 [00:01<00:01, 14011.27 examples/s]Grouping texts in chunks of 512:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 18000/36718 [00:01<00:01, 14024.23 examples/s]Grouping texts in chunks of 512:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 20000/36718 [00:01<00:01, 14293.44 examples/s]Grouping texts in chunks of 512:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 22000/36718 [00:01<00:01, 14026.41 examples/s]Grouping texts in chunks of 512:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 24000/36718 [00:01<00:00, 14526.99 examples/s]Grouping texts in chunks of 512:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 26000/36718 [00:01<00:00, 14606.45 examples/s]Grouping texts in chunks of 512:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 28000/36718 [00:01<00:00, 14053.48 examples/s]Grouping texts in chunks of 512:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 30000/36718 [00:02<00:00, 14306.52 examples/s]Grouping texts in chunks of 512:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 32000/36718 [00:02<00:00, 13778.81 examples/s]Grouping texts in chunks of 512:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 34000/36718 [00:02<00:00, 13575.31 examples/s]Grouping texts in chunks of 512:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 36000/36718 [00:02<00:00, 13656.19 examples/s]Grouping texts in chunks of 512: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 36718/36718 [00:02<00:00, 13726.43 examples/s]
Grouping texts in chunks of 512:   0%|          | 0/3760 [00:00<?, ? examples/s]Grouping texts in chunks of 512:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 2000/3760 [00:00<00:00, 13643.50 examples/s]Grouping texts in chunks of 512: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3760/3760 [00:00<00:00, 13530.69 examples/s]Grouping texts in chunks of 512: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3760/3760 [00:00<00:00, 13440.91 examples/s]
Train ControlLM len: 4656
Val ControlLM len: 250
Loading ControlLM done.
--------------------------------------------------
Logging dataset for quality check...
Dataset loading and saving done.
Prune - (out,in)_w_per_mask: (1, 1)
Replaced c_attn in h.0.attn
Replaced c_proj in h.0.attn
Replaced c_proj in h.0.mlp
Replaced c_fc in h.0.mlp
Replaced c_attn in h.1.attn
Replaced c_proj in h.1.attn
Replaced c_proj in h.1.mlp
Replaced c_fc in h.1.mlp
Replaced c_attn in h.2.attn
Replaced c_proj in h.2.attn
Replaced c_proj in h.2.mlp
Replaced c_fc in h.2.mlp
Replaced c_attn in h.3.attn
Replaced c_proj in h.3.attn
Replaced c_proj in h.3.mlp
Replaced c_fc in h.3.mlp
Replaced c_attn in h.4.attn
Replaced c_proj in h.4.attn
Replaced c_proj in h.4.mlp
Replaced c_fc in h.4.mlp
Replaced c_attn in h.5.attn
Replaced c_proj in h.5.attn
Replaced c_proj in h.5.mlp
Replaced c_fc in h.5.mlp
Replaced c_attn in h.6.attn
Replaced c_proj in h.6.attn
Replaced c_proj in h.6.mlp
Replaced c_fc in h.6.mlp
Replaced c_attn in h.7.attn
Replaced c_proj in h.7.attn
Replaced c_proj in h.7.mlp
Replaced c_fc in h.7.mlp
Replaced c_attn in h.8.attn
Replaced c_proj in h.8.attn
Replaced c_proj in h.8.mlp
Replaced c_fc in h.8.mlp
Replaced c_attn in h.9.attn
Replaced c_proj in h.9.attn
Replaced c_proj in h.9.mlp
Replaced c_fc in h.9.mlp
Replaced c_attn in h.10.attn
Replaced c_proj in h.10.attn
Replaced c_proj in h.10.mlp
Replaced c_fc in h.10.mlp
Replaced c_attn in h.11.attn
Replaced c_proj in h.11.attn
Replaced c_proj in h.11.mlp
Replaced c_fc in h.11.mlp
lambda_reg_init: 2.0, lambda_reg_final: 3.0
lr_base: 5e-05, mask_lr_base: 0.2, lr_warmup_frac: 0.1, epochs: 40000, batch_size: 250
Mask params are trainable:  True
Model params are frozen:  True


eval: : 0it [00:00, ?it/s][A[A

eval: : 1it [00:00,  1.45it/s][A[A

                              [A[A

eval: : 0it [00:00, ?it/s][A[A

                          [A[A

eval: : 0it [00:00, ?it/s][A[A

                          [A[ATraceback (most recent call last):
  File "/home/amit.vaisman/236004_transformers_project/subnet_train.py", line 242, in <module>
    main()
  File "/home/amit.vaisman/236004_transformers_project/subnet_train.py", line 214, in main
    last_log_dict, model = train_mask(
  File "/home/amit.vaisman/236004_transformers_project/know_subnet/subnet_train_utils.py", line 761, in train_mask
    log_dict = zeroshot_log_loop(
  File "/home/amit.vaisman/miniconda3/envs/236004_transformers_project/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/amit.vaisman/236004_transformers_project/know_subnet/subnet_train_utils.py", line 167, in zeroshot_log_loop
    metric_dict = test_mask(
  File "/home/amit.vaisman/miniconda3/envs/236004_transformers_project/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/amit.vaisman/236004_transformers_project/know_subnet/subnet_train_utils.py", line 89, in test_mask
    top1_acc, top5_acc, top10_acc, num_masked_tokens = acc_func(logits, labels, mask)
  File "/home/amit.vaisman/236004_transformers_project/know_subnet/metrics.py", line 72, in acc_func
    masked_prbs = prbs[mask,:]
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 11.96 GiB (GPU 0; 47.44 GiB total capacity; 43.34 GiB already allocated; 1.02 GiB free; 46.10 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "/home/amit.vaisman/236004_transformers_project/subnet_train.py", line 242, in <module>
    main()
  File "/home/amit.vaisman/236004_transformers_project/subnet_train.py", line 214, in main
    last_log_dict, model = train_mask(
  File "/home/amit.vaisman/236004_transformers_project/know_subnet/subnet_train_utils.py", line 761, in train_mask
    log_dict = zeroshot_log_loop(
  File "/home/amit.vaisman/miniconda3/envs/236004_transformers_project/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/amit.vaisman/236004_transformers_project/know_subnet/subnet_train_utils.py", line 167, in zeroshot_log_loop
    metric_dict = test_mask(
  File "/home/amit.vaisman/miniconda3/envs/236004_transformers_project/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/amit.vaisman/236004_transformers_project/know_subnet/subnet_train_utils.py", line 89, in test_mask
    top1_acc, top5_acc, top10_acc, num_masked_tokens = acc_func(logits, labels, mask)
  File "/home/amit.vaisman/236004_transformers_project/know_subnet/metrics.py", line 72, in acc_func
    masked_prbs = prbs[mask,:]
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 11.96 GiB (GPU 0; 47.44 GiB total capacity; 43.34 GiB already allocated; 1.02 GiB free; 46.10 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[1;34mwandb[0m: 
[1;34mwandb[0m: üöÄ View run [33m-date=2025_06_14_13:13:29[0m at: [34mhttps://wandb.ai/amitvais777-technion-israel-institute-of-technology/uncategorized/runs/mfliuk5v[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20250614_131330-mfliuk5v/logs[0m
*** SLURM BATCH JOB 'transformers' DONE ***
